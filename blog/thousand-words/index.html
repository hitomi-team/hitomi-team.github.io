<!doctype html><html lang=en><head><title>An Image is Worth a Thousand Words :: Lorelei Labs</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Zero-Shot Image Captioning  Allowing Dialog Language Models to approximate the content of an image through Image Captioning with the help of CLIP.  Overview We allowed Language Models like GPT-J 6B to be able to have an approximate overview of images by classifying an image with labels using natural language. Our implementation was heavily inspired through Google Research&amp;rsquo;s Socratic Models which aims to provide multimodal reasoning for Large Language Models such as GPT-3."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://hitomi-team.github.io/blog/thousand-words/><link rel=stylesheet href=https://hitomi-team.github.io/assets/style.css><link rel=apple-touch-icon href=https://hitomi-team.github.io/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://hitomi-team.github.io/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="An Image is Worth a Thousand Words"><meta property="og:description" content="Zero-Shot Image Captioning  Allowing Dialog Language Models to approximate the content of an image through Image Captioning with the help of CLIP.  Overview We allowed Language Models like GPT-J 6B to be able to have an approximate overview of images by classifying an image with labels using natural language. Our implementation was heavily inspired through Google Research&amp;rsquo;s Socratic Models which aims to provide multimodal reasoning for Large Language Models such as GPT-3."><meta property="og:url" content="https://hitomi-team.github.io/blog/thousand-words/"><meta property="og:site_name" content="Lorelei Labs"><meta property="og:image" content="https://hitomi-team.github.io/img/favicon/orange.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2022-05-13 00:00:00 +0000 UTC"></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Lorelei Labs</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about>About</a></li><li><a href=/blog>Blog</a></li><li><a href=/projects>Projects</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about>About</a></li><li><a href=/blog>Blog</a></li><li><a href=/projects>Projects</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://hitomi-team.github.io/blog/thousand-words/>An Image is Worth a Thousand Words</a></h1><div class=post-meta><span class=post-date>2022-05-13</span>
<span class=post-author>:: haru</span></div><div class=post-content><div><h2 id=zero-shot-image-captioning>Zero-Shot Image Captioning<a href=#zero-shot-image-captioning class=hanchor arialabel=Anchor>&#8983;</a></h2><ul><li>Allowing Dialog Language Models to approximate the content of an image through Image Captioning with the help of CLIP.</li></ul><h2 id=overview>Overview<a href=#overview class=hanchor arialabel=Anchor>&#8983;</a></h2><p>We allowed Language Models like GPT-J 6B to be able to have an approximate overview of images by classifying an image with labels using natural language. Our implementation was heavily inspired through Google Research&rsquo;s <a href=https://socraticmodels.github.io/>Socratic Models</a> which aims to provide multimodal reasoning for Large Language Models such as GPT-3.</p><p><img src=https://i.imgur.com/SxRLMzg.png alt=Example></p><p>Our Image Captioning system works with multiple categories of labels ranging from fictional characters to what medium the image was created in to what objects are present within the image. With more and more diverse labels to classify an image against, the Image Captioning system would be able to more accurately approximate an image into natural language. It gathers the <code>top-k</code> labels with the highest probability for all of the label sets when sorted, as erraneous labels would inevitably have the highest probability in some sets.</p><h2 id=use-in-chatbots>Use in Chatbots<a href=#use-in-chatbots class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Within our chatbots, this system allows us to explore how well of a system such as this would behave with our finetuned conversational language models such as our finetuned <a href=https://huggingface.co/hitomi-team/convo-6B>Convo-6B</a> model.</p><p><img src=https://i.imgur.com/2daXOFw.png alt="Example 2"></p><p>For our chatbots, the above message would appear like this:</p><pre tabindex=0><code>haru: @Sakuya Izayoi hey
haru: [Image Attached: Image with the Touhou Project character Tenshi Hinanawi in it.]
</code></pre><p>This style of formatting helps our model know <em>what</em> is in an image, even if the language model cannot actually see the image itself.</p><h2 id=limitations>Limitations<a href=#limitations class=hanchor arialabel=Anchor>&#8983;</a></h2><p>As mentioned above, this presents itself as a readily available solution for approximated vision with the tools that are presently available. There are numerous limitations with this technique as the only information extracted is very limited compared to what is in the image itself. After all, an image is worth a thousand words, but a more accurate saying for our use case would be <code>an image is worth a thousand words that we can't cram into our Language Models or else our local GPUs would catch fire.</code> Or, less excitingly, a megabyte long error is produced.</p><p>CLIP itself has a number of limitations as well, many of which are explained within their <a href=https://openai.com/blog/clip/>own blog post</a> and <a href=https://stanislavfort.github.io/blog/OpenAI_CLIP_stickers_and_adversarial_examples/>this one as well</a>. Essentially, CLIP is very prone to adversarial attacks where text within the image can influence the classification of an image according to CLIP.</p><p>Here is an example of that happening with our own chat bots where an image containing the phrase <code>Alice</code> lead to the label <code>Alice Margatroid</code> having a very high probability and ending up within the context sent to the Language Model:</p><p><img src=https://i.imgur.com/tRmDyDL.png alt="Example 3"></p><p><code>[Image Attached: Image with the Touhou Project character Alice Margatroid in it. This image depicts a amusement park. This is a sketch.</code></p><h2 id=conclusion>Conclusion<a href=#conclusion class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Using CLIP to classify an image with labels to approximate for chatbots is a great way to experiment on the limits of these systems without needing to do any heavy lifting ourselves. In order to mitigate against the limitations outlined above, we used several different techniques to prevent the appearance of such things while still allowing the conversation between human and machine to go smoothly.</p><p>For now, we will continue exploring this avenue of multimodal conversations with chatbots as it is something that could potentially open doors for future research in other areas outside of just computer science. We hope you enjoyed reading about our experience with Image Captioning with the help of CLIP.</p><h4 id=links>Links<a href=#links class=hanchor arialabel=Anchor>&#8983;</a></h4><ul><li><p><a href=https://discord.gg/Sx6Spmsgx7>Discord Server</a></p></li><li><p><a href=https://github.com/harubaru/eliza>The Code for the Chatbots</a></p></li><li><p><a href=https://github.com/hitomi-team/sukima>Our Backend Software</a></p></li></ul></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>Â© 2022 Lorelei Labs, Inc.</span>
<span>:: <a href=/privacy>Privacy</a> :: <a href=/terms>Terms</a></span></div></div></footer><script src=https://hitomi-team.github.io/assets/main.js></script>
<script src=https://hitomi-team.github.io/assets/prism.js></script></div></body></html>