<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blogs on Lorelei Labs</title><link>https://hitomi-team.github.io/blog/</link><description>Recent content in Blogs on Lorelei Labs</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 13 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://hitomi-team.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>An Image is Worth a Thousand Words</title><link>https://hitomi-team.github.io/blog/thousand-words/</link><pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate><guid>https://hitomi-team.github.io/blog/thousand-words/</guid><description>Zero-Shot Image Captioning Allowing Dialog Language Models to approximate the content of an image through Image Captioning with the help of CLIP. Overview We allowed Language Models like GPT-J 6B to be able to have an approximate overview of images by classifying an image with labels using natural language. Our implementation was heavily inspired through Google Research&amp;rsquo;s Socratic Models which aims to provide multimodal reasoning for Large Language Models such as GPT-3.</description><content>&lt;h2 id="zero-shot-image-captioning">Zero-Shot Image Captioning&lt;/h2>
&lt;ul>
&lt;li>Allowing Dialog Language Models to approximate the content of an image through Image Captioning with the help of CLIP.&lt;/li>
&lt;/ul>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>We allowed Language Models like GPT-J 6B to be able to have an approximate overview of images by classifying an image with labels using natural language. Our implementation was heavily inspired through Google Research&amp;rsquo;s &lt;a href="https://socraticmodels.github.io/">Socratic Models&lt;/a> which aims to provide multimodal reasoning for Large Language Models such as GPT-3.&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/SxRLMzg.png" alt="Example">&lt;/p>
&lt;p>Our Image Captioning system works with multiple categories of labels ranging from fictional characters to what medium the image was created in to what objects are present within the image. With more and more diverse labels to classify an image against, the Image Captioning system would be able to more accurately approximate an image into natural language. It gathers the &lt;code>top-k&lt;/code> labels with the highest probability for all of the label sets when sorted, as erraneous labels would inevitably have the highest probability in some sets.&lt;/p>
&lt;h2 id="use-in-chatbots">Use in Chatbots&lt;/h2>
&lt;p>Within our chatbots, this system allows us to explore how well of a system such as this would behave with our finetuned conversational language models such as our finetuned &lt;a href="https://huggingface.co/hitomi-team/convo-6B">Convo-6B&lt;/a> model.&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/2daXOFw.png" alt="Example 2">&lt;/p>
&lt;p>For our chatbots, the above message would appear like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>haru: @Sakuya Izayoi hey
haru: [Image Attached: Image with the Touhou Project character Tenshi Hinanawi in it.]
&lt;/code>&lt;/pre>&lt;p>This style of formatting helps our model know &lt;em>what&lt;/em> is in an image, even if the language model cannot actually see the image itself.&lt;/p>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>As mentioned above, this presents itself as a readily available solution for approximated vision with the tools that are presently available. There are numerous limitations with this technique as the only information extracted is very limited compared to what is in the image itself. After all, an image is worth a thousand words, but a more accurate saying for our use case would be &lt;code>an image is worth a thousand words that we can't cram into our Language Models or else our local GPUs would catch fire.&lt;/code> Or, less excitingly, a megabyte long error is produced.&lt;/p>
&lt;p>CLIP itself has a number of limitations as well, many of which are explained within their &lt;a href="https://openai.com/blog/clip/">own blog post&lt;/a> and &lt;a href="https://stanislavfort.github.io/blog/OpenAI_CLIP_stickers_and_adversarial_examples/">this one as well&lt;/a>. Essentially, CLIP is very prone to adversarial attacks where text within the image can influence the classification of an image according to CLIP.&lt;/p>
&lt;p>Here is an example of that happening with our own chat bots where an image containing the phrase &lt;code>Alice&lt;/code> lead to the label &lt;code>Alice Margatroid&lt;/code> having a very high probability and ending up within the context sent to the Language Model:&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/tRmDyDL.png" alt="Example 3">&lt;/p>
&lt;p>&lt;code>[Image Attached: Image with the Touhou Project character Alice Margatroid in it. This image depicts a amusement park. This is a sketch.&lt;/code>&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Using CLIP to classify an image with labels to approximate for chatbots is a great way to experiment on the limits of these systems without needing to do any heavy lifting ourselves. In order to mitigate against the limitations outlined above, we used several different techniques to prevent the appearance of such things while still allowing the conversation between human and machine to go smoothly.&lt;/p>
&lt;p>For now, we will continue exploring this avenue of multimodal conversations with chatbots as it is something that could potentially open doors for future research in other areas outside of just computer science. We hope you enjoyed reading about our experience with Image Captioning with the help of CLIP.&lt;/p>
&lt;h4 id="links">Links&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://discord.gg/Sx6Spmsgx7">Discord Server&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/harubaru/eliza">The Code for the Chatbots&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/hitomi-team/sukima">Our Backend Software&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></content></item><item><title>It's Definitely Not Nitro!</title><link>https://hitomi-team.github.io/blog/dnn/</link><pubDate>Sat, 07 May 2022 00:00:00 +0000</pubDate><guid>https://hitomi-team.github.io/blog/dnn/</guid><description>Announcing DNN (Definitely Not Nitro) TL;DR: The developers for Not Quite Nitro have a very misleading privacy policy that admits that their bot, Not Quite Nitro, stores sensitive information about users in the case of internal errors on their end without the user&amp;rsquo;s knowledge. We did not like their policy or practices, so we decided to make an open-source alternative.
What&amp;rsquo;s the Problem? We have a Discord server where user privacy is sacred.</description><content>&lt;h2 id="announcing-dnn-definitely-not-nitro">Announcing DNN (Definitely Not Nitro)&lt;/h2>
&lt;h2 id="tldr">TL;DR:&lt;/h2>
&lt;p>The developers for Not Quite Nitro have a very misleading privacy policy that admits that their bot, Not Quite Nitro, stores sensitive information about users in the case of internal errors on their end without the user&amp;rsquo;s knowledge. &lt;strong>We did not like their policy or practices, so we decided to make an open-source alternative.&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://i.imgur.com/7oJLLst.png" alt="An example of the devs&amp;amp;rsquo; views on user privacy.">&lt;/p>
&lt;h2 id="whats-the-problem">What&amp;rsquo;s the Problem?&lt;/h2>
&lt;p>We have a &lt;a href="https://discord.gg/Sx6Spmsgx7">Discord server&lt;/a> where user privacy is sacred. We absolutely reject the notion that a user can implicitly consent to a service&amp;rsquo;s privacy policy by simply just using it even when they are not shown the service&amp;rsquo;s privacy policy. The data that is being collected without users&amp;rsquo; explicit consent is highly sensitive and highly personal as NQN records &lt;strong>User ID, User name &amp;amp; Discriminator, and the content of messages.&lt;/strong> Even though this is being done in the event of internal errors, this practice is &lt;strong>absolutely unacceptable&lt;/strong> to us.&lt;/p>
&lt;h2 id="okay-now-whats-the-solution">Okay, now what&amp;rsquo;s the Solution?&lt;/h2>
&lt;p>We are developing a NQN alternative called &lt;strong>DNN (Definitely Not Nitro)&lt;/strong> that aims to be transparent, open source, and privacy respecting for everyone. The &lt;a href="https://hitomi-team.github.io/privacy">privacy policy&lt;/a> for our alternative basically states that absolutely no information about the user is being collected.&lt;/p>
&lt;p>You can help us out with our development by joining our &lt;a href="https://discord.gg/Sx6Spmsgx7">Discord server&lt;/a> or contributing to the &lt;a href="https://github.com/hitomi-team/DNN">main repository!&lt;/a> We gladly welcome all sorts of help as we are short handed on the development efforts for DNN.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://discord.com/api/oauth2/authorize?client_id=972542466861465620&amp;amp;permissions=536871936&amp;amp;scope=bot">Bot Invite Link&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://discord.gg/Sx6Spmsgx7">Our Discord Server&lt;/a>&lt;/li>
&lt;/ul></content></item></channel></rss>